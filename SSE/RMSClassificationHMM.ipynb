{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from preprocessEMG import train_valid_test_split, getXY\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean\n",
    "\n",
    "from HMMlearn import evaluate_hmm_model, group_training_by_class, train_hmm_models_per_user, test_hmm_models_per_user\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from exportCSV import exportCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = train_valid_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_user_hmms():\n",
    "    f1_macro_RMS = []\n",
    "    f1_micro_RMS = []\n",
    "    accuracy_RMS = []\n",
    "    f1_macro = []\n",
    "    f1_micro = []\n",
    "    accuracy = []\n",
    "    for i in range(36):\n",
    "        print(\"at user {}\".format(i))\n",
    "        trainRMSX, trainX, trainY = getXY(training[i])\n",
    "        validRMSX, validX, validY = getXY(validation[i])\n",
    "        testRMSX, testX, testY = getXY(testing[i])\n",
    "        # combine validatation and training together\n",
    "        trainRMSX.extend(validRMSX)\n",
    "        trainX.extend(validX)\n",
    "        trainY.extend(validY)\n",
    "        # classes as ints\n",
    "        trainY = [int(floatclass) for floatclass in trainY]\n",
    "        testY = [int(floatclass) for floatclass in testY]\n",
    "\n",
    "        '''\n",
    "        # get input in list of list format (RMS inputs are already as such)\n",
    "        # currently 3D - what to do?\n",
    "        trainX = np.asarray([X.values.tolist() for X in trainX])\n",
    "        testX = np.asarray([X.values.tolist() for X in testX])\n",
    "        # trainX.reshape()\n",
    "        # testX.reshape()\n",
    "        print(\"trainX shape: {}\".format(trainX.shape))\n",
    "        print(\"testX shape: {}\".format(testX.shape))\n",
    "        '''\n",
    "\n",
    "        # RMS windows\n",
    "        # train and test the model\n",
    "        predictRMSY, _, _ = evaluate_hmm_model(trainRMSX, trainY, testRMSX, testY)\n",
    "        # evaluate the model\n",
    "        f1_macro_RMS.append(f1_score(testY, predictRMSY, average='macro'))\n",
    "        f1_micro_RMS.append(f1_score(testY, predictRMSY, average='micro'))\n",
    "        accuracy_RMS.append(accuracy_score(testY, predictRMSY))\n",
    "\n",
    "        '''\n",
    "        # Raw windows\n",
    "        # train and test the model\n",
    "        predictY, _, _ = evaluate_hmm_model(trainX, trainY, testX, testY)\n",
    "        # evaluate the model \n",
    "        f1_macro.append(f1_score(testY, predictY, average='macro'))\n",
    "        f1_micro.append(f1_score(testY, predictY, average='micro'))\n",
    "        accuracy.append(accuracy_score(testY, predictY))\n",
    "        '''\n",
    "    print(\"RMS! Macro-F1: {}, Micro-F1: {}, Accuracy: {}\".format(mean(f1_macro_RMS), mean(f1_micro_RMS), mean(accuracy_RMS)))\n",
    "    #print(\"Macro-F1: {}, Micro-F1: {}, Accuracy: {}\".format(mean(f1_macro), mean(f1_micro), mean(accuracy)))\n",
    "    exportCSV(f1_macro_RMS, \"hmm_f1_macro_RMS.csv\")\n",
    "    exportCSV(f1_micro_RMS, \"hmm_f1_micro_RMS.csv\")\n",
    "    exportCSV(accuracy_RMS, \"hmm_accuracy_RMS.csv\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_hmm():\n",
    "    f1_macro_RMS = []\n",
    "    f1_micro_RMS = []\n",
    "    accuracy_RMS = []\n",
    "    f1_macro = []\n",
    "    f1_micro = []\n",
    "    accuracy = []\n",
    "    combineTrainX = []\n",
    "    combineTrainY = []\n",
    "    for i in range(36):\n",
    "        # print(\"combining data: at user {}\".format(i))\n",
    "        trainRMSX, trainX, trainY = getXY(training[i])\n",
    "        validRMSX, validX, validY = getXY(validation[i])\n",
    "        # combine validatation and training together\n",
    "        trainRMSX.extend(validRMSX)\n",
    "        trainX.extend(validX)\n",
    "        trainY.extend(validY)\n",
    "\n",
    "        combineTrainX.extend(trainRMSX)\n",
    "        combineTrainY.extend(trainY)\n",
    "        combineTrainY = [int(floatclass) for floatclass in combineTrainY]\n",
    "\n",
    "    # train\n",
    "    # Reorder train data by class\n",
    "    classes = 6\n",
    "    x_train_in, _ = group_training_by_class(classes, combineTrainX, combineTrainY)\n",
    "    # Make models for each class\n",
    "    hmm_models = train_hmm_models_per_user(classes, x_train_in)\n",
    "\n",
    "    # test the model for each user\n",
    "    for i in range(36):\n",
    "        print(\"testing: at user {}\".format(i))\n",
    "        testRMSX, testX, testY = getXY(testing[i])\n",
    "        # classes as ints\n",
    "        testY = [int(floatclass) for floatclass in testY]\n",
    "\n",
    "        # RMS windows\n",
    "        # test the model\n",
    "        # Classify each sample\n",
    "        predictRMSY = test_hmm_models_per_user(classes, testRMSX, hmm_models)\n",
    "        # evaluate the model\n",
    "        f1_macro_RMS.append(f1_score(testY, predictRMSY, average='macro'))\n",
    "        f1_micro_RMS.append(f1_score(testY, predictRMSY, average='micro'))\n",
    "        accuracy_RMS.append(accuracy_score(testY, predictRMSY))\n",
    "        #print(\"\\t RMS! Macro-F1: {}, Micro-F1: {}, Accuracy: {}\".format(mean(f1_macro_RMS), mean(f1_micro_RMS), mean(accuracy_RMS)))\n",
    "\n",
    "        '''\n",
    "        # Raw windows\n",
    "        # train and test the model\n",
    "        predictY, _, _ = evaluate_hmm_model(trainX, trainY, testX, testY)\n",
    "        # evaluate the model \n",
    "        f1_macro.append(f1_score(testY, predictY, average='macro'))\n",
    "        f1_micro.append(f1_score(testY, predictY, average='micro'))\n",
    "        accuracy.append(accuracy_score(testY, predictY))\n",
    "        '''\n",
    "    print(\"RMS! Macro-F1: {}, Micro-F1: {}, Accuracy: {}\".format(mean(f1_macro_RMS), mean(f1_micro_RMS), mean(accuracy_RMS)))\n",
    "    #print(\"Macro-F1: {}, Micro-F1: {}, Accuracy: {}\".format(mean(f1_macro), mean(f1_micro), mean(accuracy)))\n",
    "    exportCSV(f1_macro_RMS, \"hmm_f1_macro_RMS.csv\")\n",
    "    exportCSV(f1_micro_RMS, \"hmm_f1_micro_RMS.csv\")\n",
    "    exportCSV(accuracy_RMS, \"hmm_accuracy_RMS.csv\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at user 0\n",
      "at user 1\n",
      "at user 2\n",
      "at user 3\n",
      "at user 4\n",
      "at user 5\n",
      "at user 6\n",
      "at user 7\n",
      "at user 8\n",
      "at user 9\n",
      "at user 10\n",
      "at user 11\n",
      "at user 12\n",
      "at user 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olufogorehan\\PycharmProjects\\vidhyaexample\\venv\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at user 14\n",
      "at user 15\n",
      "at user 16\n",
      "at user 17\n",
      "at user 18\n",
      "at user 19\n",
      "at user 20\n",
      "at user 21\n",
      "at user 22\n",
      "at user 23\n",
      "at user 24\n",
      "at user 25\n",
      "at user 26\n",
      "at user 27\n",
      "at user 28\n",
      "at user 29\n",
      "at user 30\n",
      "at user 31\n",
      "at user 32\n",
      "at user 33\n",
      "at user 34\n",
      "at user 35\n",
      "RMS! Macro-F1: 0.8882144092111502, Micro-F1: 0.893590040089369, Accuracy: 0.893590040089369\n"
     ]
    }
   ],
   "source": [
    "evaluate_user_hmms() #GaussianHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at user 0\n",
      "at user 1\n",
      "at user 2\n",
      "at user 3\n",
      "at user 4\n",
      "at user 5\n",
      "at user 6\n",
      "at user 7\n",
      "at user 8\n",
      "at user 9\n",
      "at user 10\n",
      "at user 11\n",
      "at user 12\n",
      "at user 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olufogorehan\\PycharmProjects\\vidhyaexample\\venv\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at user 14\n",
      "at user 15\n",
      "at user 16\n",
      "at user 17\n",
      "at user 18\n",
      "at user 19\n",
      "at user 20\n",
      "at user 21\n",
      "at user 22\n",
      "at user 23\n",
      "at user 24\n",
      "at user 25\n",
      "at user 26\n",
      "at user 27\n",
      "at user 28\n",
      "at user 29\n",
      "at user 30\n",
      "at user 31\n",
      "at user 32\n",
      "at user 33\n",
      "at user 34\n",
      "at user 35\n",
      "RMS! Macro-F1: 0.8762064384472914, Micro-F1: 0.8836330576324182, Accuracy: 0.8836330576324182\n"
     ]
    }
   ],
   "source": [
    "evaluate_user_hmms() #GMMHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at user 0\n",
      "at user 1\n",
      "at user 2\n",
      "at user 3\n",
      "at user 4\n",
      "at user 5\n",
      "at user 6\n",
      "at user 7\n",
      "at user 8\n",
      "at user 9\n",
      "at user 10\n",
      "at user 11\n",
      "at user 12\n",
      "at user 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olufogorehan\\PycharmProjects\\vidhyaexample\\venv\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at user 14\n",
      "at user 15\n",
      "at user 16\n",
      "at user 17\n",
      "at user 18\n",
      "at user 19\n",
      "at user 20\n",
      "at user 21\n",
      "at user 22\n",
      "at user 23\n",
      "at user 24\n",
      "at user 25\n",
      "at user 26\n",
      "at user 27\n",
      "at user 28\n",
      "at user 29\n",
      "at user 30\n",
      "at user 31\n",
      "at user 32\n",
      "at user 33\n",
      "at user 34\n",
      "at user 35\n",
      "RMS! Macro-F1: 0.8762064384472914, Micro-F1: 0.8836330576324182, Accuracy: 0.8836330576324182\n"
     ]
    }
   ],
   "source": [
    "evaluate_user_hmms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_single_hmm() #GaussianHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing: at user 0\n",
      "testing: at user 1\n",
      "testing: at user 2\n",
      "testing: at user 3\n",
      "testing: at user 4\n",
      "testing: at user 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olufogorehan\\PycharmProjects\\vidhyaexample\\venv\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing: at user 6\n",
      "testing: at user 7\n",
      "testing: at user 8\n",
      "testing: at user 9\n",
      "testing: at user 10\n",
      "testing: at user 11\n",
      "testing: at user 12\n",
      "testing: at user 13\n",
      "testing: at user 14\n",
      "testing: at user 15\n",
      "testing: at user 16\n",
      "testing: at user 17\n",
      "testing: at user 18\n",
      "testing: at user 19\n",
      "testing: at user 20\n",
      "testing: at user 21\n",
      "testing: at user 22\n",
      "testing: at user 23\n",
      "testing: at user 24\n",
      "testing: at user 25\n",
      "testing: at user 26\n",
      "testing: at user 27\n",
      "testing: at user 28\n",
      "testing: at user 29\n",
      "testing: at user 30\n",
      "testing: at user 31\n",
      "testing: at user 32\n",
      "testing: at user 33\n",
      "testing: at user 34\n",
      "testing: at user 35\n",
      "RMS! Macro-F1: 0.7646065455329526, Micro-F1: 0.7802822452433028, Accuracy: 0.7802822452433028\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_hmm() #GMMHMM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
